\subsection{Word Classifier and Different Initialization Methods}

In this section classification results for the words in table~\ref{tab:words_supported_by_classifier} will be presented. See section~\ref{sec:method}, for a description of the implementation of the classifiers. The training and test example words were randomly generated with the a generator using the properties in table~\ref{tab:word_generator_properties}. See section~\ref{sec:dataset}, for more information about the word example generator. To test the accuracy of the created classifiers, 100 test examples were used. Five test examples for every word. The test examples were generated using the same properties as the training examples. The two initialization methods count based initialization and random initialization were tested with 100, 200, 400, 800 and 1600 training examples. The results of the test is presented in table~\ref{tab:word_classifier_results_generated_data}. It contains the test scores for the two initialization methods before and after training with the Baum-Welch algorithm. The test score is defined as the number of correctly classified test examples divided by the total number of test examples.

\begin{table}[htb]
  \begin{center}
  \begin{tabular}{ l l l l l }
    dog      & cat       & pig     & love       & hate  \\
    scala    & python    & summer  & winter     & night  \\ 
    daydream & nightmare & animal  & happiness  & sadness \\ 
    tennis   & feminism  & fascism & socialism  & capitalism \\
  \end{tabular}
\end{center}
\caption{Words supported by the resulting classifier.} 
\label{tab:words_supported_by_classifier} 
\end{table}

\begin{table}[htb]
  \begin{center}
  \begin{tabular}{ l l }
    Probability of extra letter at position         & 0.03 \\
    Probability of extra letter equal neighbor      & 0.7 \\ 
    Probability of wrong letter at position         & 0.1 \\ 
    Probability of letter missing at position       & 0.03 \\
  \end{tabular}
\end{center}
\caption{Properties obeyed by the word training example generator.} 
\label{tab:word_generator_properties} 
\end{table}

\begin{table}[htb]
  \begin{center}
  \begin{tabular}{ l l l l l l l }
    NOE    & RIBF   & CBIBT  & RIAT    & CBIAT  & RITT & CBITT\\ \hline
    $100$  & $0.03$ & $0.99$ & $0.01$  & $0.0$  & $6$  & $3$\\ 
    $200$  & $0.02$ & $1.0$  & $0.13$  & $0.36$ & $12$ & $5$\\ 
    $400$  & $0.02$ & $1.0$  & $0.9$   & $0.95$ & $23$ & $7$\\
    $800$  & $0.01$ & $1.0$  & $1.0$   & $1.0$  & $46$ & $13$\\   
    $1600$ & $0.05$ & $1.0$  & $1.0$   & $1.0$  & $96$ & $26$\\  
  \end{tabular}
\end{center}
\caption{Test with different number of training examples and different initialization methods.
	 NOE=''number of training examples for every word'',
         RIBF=''random initialization score before training'',
         CBIBT=''count based initialization score before training'',
         RIAT=''random initialization score after training'',
         CBIAT=''count based initialization score after training'',
         RITT=''random initialization training time (minutes)'',
         CBITT=''count based initialization training time (minutes)''} 
\label{tab:word_classifier_results_generated_data} 
\end{table}

\subsection{Character Classification With Different Parameters}

As described in section~\ref{sec:image_preprocessing} the image feature extraction step in the character classifier takes two parameters. The frist parameter is the number of segments that shall be created. The second parameter is the size classification factor which is defined in equation~\ref{eq:classification_function} which can be found in section~\ref{sec:image_preprocessing}. For the experiment we only had 100 examples for each of the 26 characters. How the examples are produced is described in section~\ref{sec:dataset}. An inital experiment that was performed with count based initalisation and random initalisation before and after training with the Baum-Welch algorithm shows that there is probably to few training examples for training to have any positive effect for count based intitalisation. This could possible be fixed with some kind of smothening of the model produced by the training, but the time constraints of the project prevented us from testing that. Ten test examples and 90 training examples for every character were selected randomly from the example set for the experiments. The results from the inital experiment can be found in table~\ref{tab:character_classifier_initial_experiment}. In the inital experiment $1.3$ was used as the size classification factor and the number of segments was set to $7$.


\begin{table}[htb]
  \begin{center}
  \begin{tabular}{ l l l l l }
    NOE    & RIBF   & CBIBT  & RIAT    & CBIAT \\ \hline
    $90$  & $0.04$ & $0.53$ & $0.16$  & $0.16$  \\   
  \end{tabular}
\end{center}
\caption{Test of the character classifier with different initialization methods and before and after training.
	 NOE=''number of training examples for every word'',
         RIBF=''random initialization score before training'',
         CBIBT=''count based initialization score before training'',
         RIAT=''random initialization score after training'',
         CBIAT=''count based initialization score after training''} 
\label{tab:character_classifier_initial_experiment} 
\end{table}

Only count based intialisation is looked upon in the experiment of diffenrent parameters, because the inital experiment showed that the best result seems to be given when just using count based intitalisation and no training. When testing the parameters, 5 models were created in the same way as described for the intial experiment. The the average accuracy of these models when testing them with their own exaple set was recorded as the accuracy for the configuration. For all 5 modesl that were created with the same parameters different training example sets and test example sets were randomly selected. 90 training examples and 10 test examples were used as in the inital experiment. The results of experiment can be seen in table~\ref{tab:character_classifier_results_different_parameters}.

\begin{table}[htb]
  \begin{center}
  \begin{tabular}{ l | l l l l l l l l l }
CCF/NOS &    4   & 5      & 6      & 7      & 8      & 9      & 10     & 11     & 12 \\ \hline
0.7     &  $32$  &  $38$  &  $43$  &  $42$  &  $44$  &  $44$  &  $45$  &  $48$  &  $48$  \\
1.0     &  $35$  &  $39$  &  $42$  &  $41$  &  $44$  &  $49$  &  $48$  &  $47$  &  $48$  \\
1.3     &  $43$  &  $47$  &  $53$  &  $53$  &  $53$  &  $56$  &  $57$  &  $56$  &  $59$  \\
1.6     &  $47$  &  $49$  &  $53$  &  $55$  &  $57$  &  $57$  &  $58$  &  $57$  &  $57$  \\
1.9     &  $50$  &  $52$  &  $56$  &  $57$  &  $58$  &  $57$  &  $58$  &  $59$  &  $59$  \\
2.2     &  $55$  &  $57$  &  $60$  &  $60$  &  $62$  &  $62$  &  $62$  &  $65$  &  $63$  \\
2.5     &  $57$  &  $59$  &  $60$  &  $61$  &  $62$  &  $62$  &  $65$  &  $65$  &  $63$  \\
2.8     &  $55$  &  $60$  &  $61$  &  $62$  &  $64$  &  $65$  &  $66$  &  $64$  &  $65$  \\
3.1     &  $55$  &  $59$  &  $64$  &  $63$  &  $65$  &  $66$  &  $65$  &  $67$  &  $68$  \\
3.4     &  $56$  &  $60$  &  $62$  &  $64$  &  $64$  &  $65$  &  $65$  &  $67$  &  $65$  \\
3.7     &  $55$  &  $62$  &  $62$  &  $64$  &  $63$  &  $64$  &  $67$  &  $67$  &  $67$  \\
4.0     &  $56$  &  $61$  &  $61$  &  $65$  &  $64$  &  $64$  &  $66$  &  $66$  &  $65$  \\
4.3     &  $56$  &  $61$  &  $63$  &  $65$  &  $65$  &  $65$  &  $66$  &  $67$  &  $66$  \\
4.6     &  $55$  &  $60$  &  $62$  &  $65$  &  $65$  &  $65$  &  $67$  &  $68$  &  $66$  \\
4.9     &  $55$  &  $60$  &  $63$  &  $67$  &  $67$  &  $66$  &  $67$  &  $67$  &  $66$  \\
5.2     &  $54$  &  $60$  &  $63$  &  $65$  &  $64$  &  $67$  &  $66$  &  $65$  &  $65$  \\
5.5     &  $53$  &  $59$  &  $63$  &  $63$  &  $66$  &  $66$  &  $67$  &  $67$  &  $66$  \\
5.8     &  $51$  &  $60$  &  $63$  &  $65$  &  $63$  &  $67$  &  $68$  &  $66$  &  $66$  \\
  \end{tabular}
\end{center}
\caption{Results for character classification test with different parameters. The scores are percentage of correctly classified characters.
	 NOS=''number of segments'',
         CCF=''component classification factor''}
\label{tab:character_classifier_results_different_parameters} 
\end{table}
