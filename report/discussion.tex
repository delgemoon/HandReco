In section~\ref{sec:word_classifier_results} the test results for the word classifier is presented.
From these results it is possible to see the effect of not having enough training data.
The count based initialization method got worse accuracy after training the model with Baum-Welch for all test runs with less than 800 training examples.
The effect of not having enough training data when using the Baum-Welch algorithm is discussed in~\cite{Rabiner1989}.
This could be dealt with by doing some kind of smoothening of the probability matrices after training.
The smoothening could be done by for example setting all transitions with probability zero to a small value greater than zero.
The count based initialization method gives almost perfect accuracy on the test set without the Baum-Welch training.
The vocabulary that the classifier supports is quite small with only 20 words.
The accuracy would probably be worse with a larger vocabulary that have many words that are similar to each other.
The classifier implementation would also have permanence problems for many applications with large vocabularies.
This is because the time complexity of classifying an example grows linearly with the size of the vocabulary.
This is easy to see if one consider that the classifier contains one HMM for every word in the vocabulary and that the forward calculation algorithm needs to run for all HMMs when an example shall be classified.
It is also possible to question the usefulness of the word classifier when the training examples are generated as they have been in the test.
For most applications it would be better to use a spell-checking algorithm that can find words similar to a string in an effective way.
However, if the training data instead is the result of a handwritten recognition system it could be more useful, because then the model could learn to correct mistakes that the handwritten recognition system does.

In section~\ref{sec:character_classifier_results} the test results for the character classifier is described.
For the character classifier the amount of available training examples is probably a limiting factor, since training with the Baum-Welch algorithm gives worse accuracy than without any training, when using the count based initialization method.
If the classifier shall be accurate for a random persons handwriting it would be beneficial to let more people paint training examples to get more variation.
Our approach will always have problem with characters that look similar to other characters when turned upside down.
For example ''M'' and ''W'' look exactly like the other if they are turned upside down for some handwriting styles.
Why this problem occur is obvious if one looks at the feature extraction process.
One way to improve this would be to not only do segmentation from left to right but also from top to bottom.